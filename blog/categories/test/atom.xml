<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: test | Steve Hostettler]]></title>
  <link href="http://hostettler.github.io/blog/categories/test/atom.xml" rel="self"/>
  <link href="http://hostettler.github.io/"/>
  <updated>2014-05-22T21:37:57+02:00</updated>
  <id>http://hostettler.github.io/</id>
  <author>
    <name><![CDATA[Steve Hostettler]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Verification & Validation]]></title>
    <link href="http://hostettler.github.io/blog/2012/05/18/verification-validation-and-all-that/"/>
    <updated>2012-05-18T10:51:00+02:00</updated>
    <id>http://hostettler.github.io/blog/2012/05/18/verification-validation-and-all-that</id>
    <content type="html"><![CDATA[<p>I've read many articles (tutorials, books, courses, forums...) about
Verification &amp; Validation. One think that really strikes me is the lack of consistency and the
fuzziness among the definitions or more precisely among the interpretations. As I do think that
something cannot be understood completely if it is not be expressed clearly, I propose a mini-serie of blog posts to clarify Verification &amp; Validation and the associated
activities. I will try to give simple and consistent (at least in my point of view) definitions. Sure, this is a rehash of well-known stuff but putting everything at the same place seems worth to me. I do not pretend to know best and thus feel free to comment on this post to correct me or to suggest other explanations and definitions. My only purpose is to help to better understand these words and the underlying concepts.</p>

<h2>Verification &amp; Validation</h2>

<p>Most of the literature on the subject distinguish between <strong>Verification</strong> and <strong>Validation</strong>. The
usual way (taken from Barry Boehm) to sum up the difference between the two is to say that:</p>

<p><code>Verification  consist in checking that we are building the product right, and validation
 consists in checking building the right product.</code></p>

<p>I am not very satisfied with this definition. Although it sounds good, it raises the question of
what it does mean to built a product right and what the right product is. Let's go back to the roots
by looking at the definitions of these words in the <a href="http://www.merriam-webster.com/">Merriam-Webster</a> dictionary.</p>

<!---
###Oxford dictionary
**Verification**: the process of establishing the truth, accuracy, or validity of something.  
**Validation**: check or prove the validity or accuracy of something.

###Cambridge dictionary
**Verification**: to prove that something exists or is true, or to make certain that something is correct.  
**Validation**: to make something officially acceptable or approved, especially after examining it.
-->


<p><strong>Verification</strong>: the act or process of "establishing the truth, accuracy, or reality of a claim".<br/>
<strong>Validation</strong>: the act or process of "making legally valid".</p>

<!--- All three dictionaries agree (as far as I understand it) on the definition of verification.
The 
[Oxford] (http://oxforddictionaries.com/) dictionary does not really distinguish the two words.

[Cambridge] (http://dictionary.cambridge.org/) and [Merriam-Webster]
(http://www.merriam-webster.com/), on the other hand, describe **validation** as the act of making
something legal or official. As French is my mother tongue and both **verification** and **validation**
come from the middle French, I checked the definitions in a French dictionary ([Larousse](http://www.larousse.com/en/dictionaries/french/)).
The Larousse's definition also makes a clear distinction that is along the same lines as the Cambridge 
and the Merriam-Webster dictionary. 
-->


<p>To summarize, my understanding is that the general sense of <strong>verification</strong> is to prove a claim
whereas <strong>validation</strong> aims at declaring something legal or official (i.e. something that comply to a
set of rules or regulations). Of course this raises the question of what, in the context of software
systems, is a claim and what is legal/official. This leads us to the next section that defines terms
such as requirements, and specification.</p>

<h2>Requirements &amp; Specification</h2>

<!---
###Oxford dictionary
**Requirement**:   
**Specification**: 

###Cambridge dictionary
**Requirement**:   
**Specification**: 
-->


<p>Again, let's lookup the meaning of these words in the <a href="http://www.merriam-webster.com/">Merriam-Webster</a> dictionary:</p>

<p><strong>Requirement</strong>: the need for a particular purpose; depend on for success or survival.<br/>
<strong>Specification</strong>: the act of describing or identifying something precisely or of stating a precise requirement.</p>

<p>A requirement expresses a need upon which the success of the project depends. Whereas, a
specification is a precise description of a requirement. Now let us look at the standard definitions
in Software Engineering.</p>

<h2>Software Requirements &amp; Specification</h2>

<p>As a software systems is usually developed in response to someone needs (or supposed needs), it
must satisfy that person, that group of person, or that organization needs. Furthermore, it must
often comply to a set of regulations or other expectations. This leads us to the following concept:</p>

<p><strong>Requirements</strong>  represent what the stakeholders expect from the system. It is important to note
   that the stakeholders can be end-users, power-users or even internal or external regulatory
   entities such as government agencies that enforce laws and regulations. Stakeholder requirements
   do not take the feasibility into account nor do they take technical details into account. They
   only state their problem or expectations.</p>

<p>As it is not possible to have access to all stakeholders at any time during the development process
(for instance, to ask questions), it is necessary to capture their requirements in a persistent form.
In the following, I call the role that capture the stakeholder requirements: the business analyst.</p>

<p><strong>Specifications</strong>  capture in a written form what a business analyst understood from the
 requirements. This written form is usually expressed in natural language or is a mixed of natural
 language and semi-formal, or formal description. The specifications describe implementable requirements and how to meet them. Therefore, they propose a solution to the problem.</p>

<p>Interestingly enough, there is the same kind of semantical difference between <strong>requirements</strong> and <strong>specifications</strong>
as between <strong>verification</strong> and <strong>validation</strong>. This is the basis that helps to understand the subtle
yet important difference between the two terms, in particular in the context of the development of
software systems.</p>

<p> It is important to understand is that the specifications are derived from the interpreted <strong>requirements</strong>
 and therefore <strong>specifications</strong> cannot be sound and complete. In other words, there may be things
 that are over-specified (not sound) and things that are under-specified (not complete).</p>

<p>Let's imagine for a second that a perfect business analyst did capture exactly what the
stakeholders had in mind. This perfect specification must still be implemented and therefore we must
ensure that implementation does satisfy the specification (and by transitivity the stakeholders
needs).</p>

<p>A software system does reflect the stakeholder requirements if the following hypotheses hold:</p>

<ol>
<li>All stakeholders expresses their needs in a sound and complete set of requirements;</li>
<li>a business analyst captures these requirements in a sound and complete specification (with respect to the requirements);</li>
<li>the development team understands and implements this specification in a sound and complete way;</li>
</ol>


<p>Of course non of these hypotheses hold in a real industrial setting but we have to admit them to be
able to realize a system. In order to minimize the risk of diverging from the stakeholders
requirements to much we need a process to ensure that the final system agrees to a certain degree to
the stakeholders requirements. This a the role of verification and validation.</p>

<h2>Software Verification &amp; Validation</h2>

<p>The figure below illustrates the difference between <strong>verification</strong> and <strong>validation</strong> and where
they take place in the development process. Building a software system consists in capturing the
stakeholders' requirements into a persistent description, called <strong>informal specifications</strong> (e.g.,
Use Cases, storyboards, ...). This specifies <strong>what</strong> problems the system addresses. At this stage a
number of interpretation mistakes may have occurred leading to incorrect specifications. To limit
these problems, the <strong>informal specifications</strong> must be explained to the user and validated.
Developers require non-ambiguous specification to work on. Thus, a design (e.g., UML diagrams) are
derived from the <strong>informal specifications</strong> . Similarly, the quality assurance team derives a <strong>formal
specification</strong> (e.g, Petri nets, test cases, metrics, ...). The design and later the implementation
are then verified with respect to this <strong>formal specification</strong>. To a certain extend, the formal specification
must itself be validated by the stakeholders.</p>

<p><span class='caption-wrapper center'><img class='caption' src='/figures/V&amp;V.png' width='' height='' alt='Verification and Validation' title='Verification and Validation'><span class='caption-text'>Verification and Validation</span></span></p>

<p><strong>Verification</strong>    stands for the process of evaluating whether a software system satisfies the <strong>specified
requirements</strong>. On the design level, it can be achieved by simulation or model checking. On the implementation
   level, it can be achieved by different kind of testing, code review, static analysis, and
   metrics. In other words: "Did we build the system right (with respect to specified
   requirements)?".</p>

<p><strong>Validation</strong>    stands for the process of evaluating whether a software system satisfies the <strong>stakeholder requirements</strong>
   and <strong>intended uses</strong> . This is, for instance, achieved by <strong>User Acceptance Tests</strong> and by
   submitting the final system to certification authorities. In other words: "Did we build the right
   (with respect to stakeholder requirements) system?".</p>

<!---
Validation is necessary because there will always be a gap (or discrepancy) between what the customer wants, and what has been captured and expressed in the requirements. There is inevitably some loss, or corruption, of information in its transmission between the customer and the analyst/developer.
The essential point is that the customer or user must be directly involved in validation.
--->


<h2>Conclusion</h2>

<p>Stakeholders cannot be available at any time of the development. Therefore, it is necessary to
build a persistent view (i.e., specification) of their needs (i.e., requirements). The activity of
checking whether the design and the implementation fulfill the specification is called
<strong>Verification</strong>
Some details may be lost in the translation from the requirements to the specification. Therefore,
at each phase of the development of a software systems, we must check that what has been specified
and later implemented during that phase reflect the stakeholders' expectations. This activity is
called <strong>validation</strong> . Integrating these two concepts as first class citizen, is one of the major
benefits of agile methods. Because the interpretation of the <strong>stakeholder requirements</strong> and later
the translation from the <strong>informal specifications</strong> to the <strong>formal specifications</strong> may be
incomplete or incorrect, it is necessary to often go back to the customer or the stakeholders to
confirm that the product is on the good track. This is one of the major benefits of iterative and
incremental development.</p>

<!---
The [Food and Drug Administration]() 

**Software verification**  provides objective evidence that the design outputs of a particular phase
 of the software development life cycle meet all of the specified requirements for that phase.
 Software verification looks for consistency, completeness, and correctness of the software and its
 supporting documentation, as it is being developed, and provides support for a subsequent
 conclusion that software is validated. Software testing is one of many verification activities
 intended to confirm that software development output meets its input requirements. Other
 verification activities include various static and dynamic analyses, code and document inspections,
 walkthroughs, and other techniques.

**Software validation**  is a part of the design validation for a finished device, but is not
 separately defined in the Quality System regulation. For purposes of this guidance, FDA considers
 software validation to be ``confirmation by examination and provision of objective evidence that
 software specifications conform to user needs and intended uses, and that the particular
 requirements implemented through software can be consistently fulfilled.'' In practice, software
 validation activities may occur both during, as well as at the end of the software development life
 cycle to ensure that all requirements have been fulfilled. Since software is usually part of a
 larger hardware system, the validation of software typically includes evidence that all software
 requirements have been implemented correctly and completely and are traceable to system
 requirements. A conclusion that software is validated is highly dependent upon comprehensive
 software testing, inspections, analyses, and other verification tasks performed at each stage of
 the software development life cycle. Testing of device software functionality in a simulated use
 environment, and user site testing are typically included as components of an overall design
 validation program for a software automated device.

IEEE-829-2008 3.1.53 validation: (A) The process of evaluating a system or component during or at
the end of the development process to determine whether it satisfies specified requirements.
(adopted from IEEE Std 610.12-1990 [B3] ) (B) The process of providing evidence that the software and
its associated products satisfy system requirements allocated to software at the end of each life
cycle activity, solve the right problem (e.g., correctly model physical laws, implement business
rules, or use the proper system assumptions), and satisfy intended use and user needs.

3.1.54 verification: (A) The process of evaluating a system or component to determine whether the
products of a given development phase satisfy the conditions imposed at the start of that phase.
(adopted from IEEE Std 610.12-1990 [B3] ) (B) The process of providing objective evidence that the
software and its associated products comply with requirements (e.g., for correctness, completeness,
consistency, and accuracy) for all life cycle activities during each life cycle process
(acquisition, supply, development, operation, and maintenance), satisfy standards, practices, and
conventions during life cycle processes, and successfully complete each life cycle activity and
satisfy all the criteria for initiating succeeding life cycle activities (e.g., building the
software correctly).


1. [IEEE-829-2008]()
2. [ISTQB foundation]() and [advanced]() syllabus
3. [ISO 9000:2000]()
4. [ISO 8402:1994]()
5. [ISO/IEC 14971-1 and IEC 60601-1-4]() 
6. [General Principles of Software Validation; Final Guidance for Industry and FDA Staff]()
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Embedded Web Application integration testing using Tomcat 7 and JUnit]]></title>
    <link href="http://hostettler.github.io/blog/2012/04/09/embedded-jee-web-application-integration-testing-using-tomcat-7/"/>
    <updated>2012-04-09T14:39:00+02:00</updated>
    <id>http://hostettler.github.io/blog/2012/04/09/embedded-jee-web-application-integration-testing-using-tomcat-7</id>
    <content type="html"><![CDATA[<p>To follow up on my previous post about <a href="/blog/2012/04/05/programmatically-build-web-archives-using-shrinkwrap/">how to programmatically build a web archive</a>, I propose to look at how to deploy this archive in a Tomcat instance that is embedded into a JUnit test.</p>

<p>As usual, the presented snippets are available in the <a href="http://code.google.com/p/jee6-demo/">JEE-6-Demo</a> application. I will first look at the dependencies, then I will show how to configure Tomcat and finally how to start and stop the embedded instance.</p>

<p>The first dependency is the embedded Tomcat core component. JULI which stands for Java Utility Logging Implementation that is the container extension of common logging. The ECJ compiler and JASPER are required to handle JSPs. Of course all these dependencies are scoped to test only.</p>

<h2>The dependencies</h2>

<p>```xml Maven dependencies necessary to start an embedded Tomcat instance into JUnit tests.
<dependency></p>

<pre><code>&lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt;
&lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt;
&lt;version&gt;7.0.26&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
<dependency></p>

<pre><code>&lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt;
&lt;artifactId&gt;tomcat-embed-logging-juli&lt;/artifactId&gt;
&lt;version&gt;7.0.26&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
<dependency></p>

<pre><code>&lt;groupId&gt;org.eclipse.jdt.core.compiler&lt;/groupId&gt;
&lt;artifactId&gt;ecj&lt;/artifactId&gt;
&lt;version&gt;3.7.1&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
<dependency></p>

<pre><code>&lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt;
&lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt;
&lt;version&gt;7.0.26&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
<dependency></p>

<pre><code>&lt;groupId&gt;junit&lt;/groupId&gt;
&lt;artifactId&gt;junit&lt;/artifactId&gt;
&lt;version&gt;4.10&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
```</p>

<h2>Configuring Tomcat</h2>

<p>The following snippet prepares the embedded instance. The static member <code>mTomcat</code> references a new instance of Tomcat. As Tomcat still requires a file system to work, a temporary directory is used to put all the temporary files. To that end, we use the <code>java.io.tmpdir</code> java property.
These variables are shared among the test cases.</p>

<p>```java Shared static variables</p>

<p>/<strong> The tomcat instance. */
private Tomcat mTomcat;
/</strong> The temporary directory in which Tomcat and the app are deployed. */
private String mWorkingDir = System.getProperty("java.io.tmpdir");
```</p>

<p>As we want to use the same Tomcat configuration among all test cases, the initialization is put in a method annotated with <code>@Before</code>. First, the method sets the port to <code>0</code>. This tells the engine to choose the port to run on by itself.  This is especially useful to avoid  starting the embedded Tomcat on a already used port as <code>8080</code> for instance.
Then the base directory is set <code>mTomcat.setBaseDir()</code> to the temporary directory. Without doing that, Tomcat would start in the current directory. The rest of the method configures the way WAR are managed by the engine.</p>

<p>```java Configuration of the embedded Tomcat instance
@Before
public void setup() throws Throwable {</p>

<pre><code>mTomcat = new Tomcat();
mTomcat.setPort(0);
mTomcat.setBaseDir(mWorkingDir);
mTomcat.getHost().setAppBase(mWorkingDir);
mTomcat.getHost().setAutoDeploy(true);
mTomcat.getHost().setDeployOnStartup(true);
...
</code></pre>

<p>}
```</p>

<p>The rest of the method builds a reference to a directory (based into the temporary directory) that contains the web application. This directory is deleted if it exists to ensure redeployment of the WAR. Finally, the WAR built as explained <a href="/blog/2012/04/05/programmatically-build-web-archives-using-shrinkwrap/">there</a> is exported into the temporary directory.</p>

<p>Finally, the web application is added to the Tomcat instance. More specifically, a path
exploded version <code>webApp.getAbsolutePath()</code> of the WAR is linked to a context <code>contextPath</code>.</p>

<p>```java Cleaning and preparing the web application deployment.
@Before
public void setup() throws Throwable {</p>

<pre><code>...
String contextPath = "/" + getApplicationId();
File webApp = new File(mWorkingDir, getApplicationId());
File oldWebApp = new File(webApp.getAbsolutePath());
FileUtils.deleteDirectory(oldWebApp);
new ZipExporterImpl(createWebArchive()).exportTo(new File(mWorkingDir + "/" + getApplicationId() + ".war"),
        true);
mTomcat.addWebapp(mTomcat.getHost(), contextPath, webApp.getAbsolutePath());    
</code></pre>

<p>}
```</p>

<h2>Starting Tomcat</h2>

<p>Now that Tomcat has been configured, the next step is to start it. We want a fresh Tomcat
for each and every test case. This way, a failed test does not have repercussions on the subsequent tests (e.g. session information, memory leaks);</p>

<p>```java Start the embedded Tomcat instance and add a web application
@Before
public void setup() throws Throwable {</p>

<pre><code>...
mTomcat.start();
</code></pre>

<p>}
```</p>

<p>If necessary, the actual port on which Tomcat has been started can be retrieved using the following snippet.
```java
protected int getTomcatPort() {</p>

<pre><code>return mTomcat.getConnector().getLocalPort();
</code></pre>

<p>}
```</p>

<p>Finally, after the test, the Tomcat instance is stopped and destroyed clearing the way for the next test.</p>

<h2>Stopping Tomcat</h2>

<p>```java
@After
public final void teardown() throws Throwable {</p>

<pre><code>if (mTomcat.getServer() != null
        &amp;&amp; mTomcat.getServer().getState() != LifecycleState.DESTROYED) {
    if (mTomcat.getServer().getState() != LifecycleState.STOPPED) {
            mTomcat.stop();
    }
    mTomcat.destroy();
}
</code></pre>

<p>}
```</p>

<h2>Conclusion</h2>

<p>In this post, we have seen how to start an embedded version of Tomcat into a JUnit fixture.
The examples used in this blog are to be found in the <a href="http://code.google.com/p/jee6-demo/">JEE-6-Demo</a> project on <a href="http://code.google.com">Google code hosting</a>.</p>
]]></content>
  </entry>
  
</feed>
